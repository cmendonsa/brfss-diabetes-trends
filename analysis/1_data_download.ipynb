{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615d6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import gc\n",
    "import tracemalloc\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any, Union, Callable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm.notebook import tqdm  # Professional progress bars\n",
    "\n",
    "# Configure Logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fb92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YEARS = list(range(2015, 2025))\n",
    "\n",
    "# Use pathlib for robust path handling\n",
    "BASE_DIR = Path.cwd().parent \n",
    "DATA_DIR = BASE_DIR / \"data_raw\"\n",
    "PROCESSED_DATA_DIR = BASE_DIR / \"data_processed\"\n",
    "CONFIG_DIR = BASE_DIR / \"config\"\n",
    "\n",
    "# Ensure directories exist\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# URL Pattern: Note that CDC naming conventions may vary by year.\n",
    "# This pattern assumes a consistent structure.\n",
    "BASE_URL = \"https://www.cdc.gov/brfss/annual_data/{year}/files/LLCP{year}XPT.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcaf535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:13:20 - INFO - Loading configuration from: c:\\github\\brfss-diabetes-trends\\config\\VAR_MAP.json\n",
      "20:13:20 - INFO - Loading configuration from: c:\\github\\brfss-diabetes-trends\\config\\VALUE_MAP.json\n",
      "20:13:20 - INFO - Loading configuration from: c:\\github\\brfss-diabetes-trends\\config\\VALUE_TEXT_MAP.json\n",
      "20:13:20 - INFO - Mappings loaded successfully.\n",
      "20:13:20 - INFO - - VAR_MAP: 20 fields\n",
      "20:13:20 - INFO - - VALUE_MAP: 20 fields\n",
      "20:13:20 - INFO - - VALUE_TEXT_MAP: 20 fields\n"
     ]
    }
   ],
   "source": [
    "def load_json_config(path: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Loads a JSON configuration file from the specified path.\n",
    "\n",
    "    Args:\n",
    "        path (Path): The file system path to the JSON configuration file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The parsed JSON data as a dictionary.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the file does not exist at the specified path.\n",
    "        json.JSONDecodeError: If the file content is not valid JSON.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {path}\")\n",
    "    \n",
    "    logger.info(f\"Loading configuration from: {path}\")\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def parse_mapping_config(data: Dict, key_type: type = str, inner_key_type: type = str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parses a raw mapping dictionary, casting keys to specific types (e.g., converting year strings to ints).\n",
    "\n",
    "    This helper function is designed to normalize the keys of nested or flat dictionaries\n",
    "    loaded from JSON, ensuring consistent types (like `int` for years or codes) across the application.\n",
    "\n",
    "    Args:\n",
    "        data (Dict): The raw dictionary loaded from JSON.\n",
    "        key_type (type, optional): The target type for the top-level keys. Defaults to str.\n",
    "        inner_key_type (type, optional): The target type for inner dictionary keys \n",
    "            (used for nested structures like `VALUE_MAP`). Defaults to str.\n",
    "\n",
    "    Returns:\n",
    "        Dict: A new dictionary with keys cast to the specified types.\n",
    "    \"\"\"\n",
    "    parsed = {}\n",
    "    for canonical, mappings in data.items():\n",
    "        if isinstance(mappings, dict):\n",
    "             # Handle nested dictionaries (e.g., VAR_MAP, VALUE_MAP)\n",
    "             # Logic: Cast top-level key -> Check if value is dict -> Cast inner keys if so\n",
    "            parsed[canonical] = {\n",
    "                (int(k) if key_type == int else k): (\n",
    "                    {int(ik): iv for ik, iv in v.items()} if isinstance(v, dict) and inner_key_type == int else v\n",
    "                )\n",
    "                for k, v in mappings.items()\n",
    "            }\n",
    "        else:\n",
    "            # Handle flat dictionaries (e.g., VALUE_TEXT_MAP)\n",
    "            parsed[canonical] = {int(k): v for k, v in mappings.items()}\n",
    "    return parsed\n",
    "\n",
    "# Load and Parse Configurations\n",
    "try:\n",
    "    _var_data = load_json_config(CONFIG_DIR / \"VAR_MAP.json\")\n",
    "    # VAR_MAP structure: {canonical: {year (int): col_name (str)}}\n",
    "    VAR_MAP = parse_mapping_config(_var_data, key_type=int) \n",
    "    \n",
    "    _val_data = load_json_config(CONFIG_DIR / \"VALUE_MAP.json\")\n",
    "    # VALUE_MAP structure: {canonical: {year (int): {code (int): label (str)}}}\n",
    "    VALUE_MAP = parse_mapping_config(_val_data, key_type=int, inner_key_type=int) \n",
    "    \n",
    "    _text_data = load_json_config(CONFIG_DIR / \"VALUE_TEXT_MAP.json\")\n",
    "    # VALUE_TEXT_MAP structure: {canonical: {code (int): label (str)}}\n",
    "    VALUE_TEXT_MAP = parse_mapping_config(_text_data, key_type=int) \n",
    "\n",
    "    logger.info(f\"Mappings loaded successfully.\")\n",
    "    logger.info(f\"- VAR_MAP: {len(VAR_MAP)} fields\")\n",
    "    logger.info(f\"- VALUE_MAP: {len(VALUE_MAP)} fields\")\n",
    "    logger.info(f\"- VALUE_TEXT_MAP: {len(VALUE_TEXT_MAP)} fields\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load configurations: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009c4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_xpt(year, retries: int = 3, timeout: int = 30):\n",
    "    \"\"\"\n",
    "    Ensures that the LLCP{year}.XPT file exists locally, downloading and extracting it if necessary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1.  **Download:** If the source ZIP file is missing, it is downloaded from `BASE_URL` \n",
    "        using exponential backoff for retries.\n",
    "    2.  **Extraction:** If the XPT file is missing, the function locates the first `.xpt` \n",
    "        file within the ZIP archive (case-insensitive).\n",
    "    3.  **Stabilization:** It extracts the file, handles potential whitespace in the \n",
    "        filename, and waits for the file size to stabilize (ensuring extraction is \n",
    "        complete) before moving it to the final destination.\n",
    "\n",
    "    Args:\n",
    "        year (int): The target year to fetch (e.g., 2019).\n",
    "        retries (int, optional): The maximum number of download attempts. Defaults to 3.\n",
    "        timeout (int, optional): The request timeout in seconds for each attempt. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        str: The full filesystem path to the local .XPT file.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the ZIP archive does not contain any .xpt files.\n",
    "        zipfile.BadZipFile: If the local ZIP file is corrupt or invalid.\n",
    "        requests.exceptions.RequestException: If the download fails after all retries are exhausted.\n",
    "    \"\"\"\n",
    "    DATA_PARENT_DIR = os.path.join(DATA_DIR, f\"{year}\")\n",
    "    if not os.path.exists(DATA_PARENT_DIR):\n",
    "        os.makedirs(DATA_PARENT_DIR, exist_ok=True)\n",
    "    zip_path = os.path.join(DATA_PARENT_DIR, f\"LLCP{year}.zip\")\n",
    "    xpt_path = os.path.join(DATA_PARENT_DIR, f\"LLCP{year}.XPT\")\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"[DOWNLOAD] {year}\")\n",
    "        url = BASE_URL.format(year=year)\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(zip_path, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Download attempt {attempt} failed for {year}: {e}\")\n",
    "                if attempt < retries:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    raise\n",
    "    else:\n",
    "        print(f\"[CACHED] {year} zip\")\n",
    "\n",
    "    if not os.path.exists(xpt_path):\n",
    "        print(f\"[UNZIP] {year}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                # Find the .xpt file (case insensitive, strip whitespace)\n",
    "                all_files = z.namelist()\n",
    "                members = [m for m in all_files if m.lower().strip().endswith(\".xpt\")]\n",
    "                if not members:\n",
    "                    print(f\"DEBUG: Contents of zip {year}: {all_files}\")\n",
    "                    raise RuntimeError(f\"No .xpt found in zip for {year}\")\n",
    "                \n",
    "                # extract first .xpt and move it to expected path\n",
    "                member = members[0]\n",
    "                member_stripped = member.strip()  # Remove leading/trailing whitespace\n",
    "                print(f\"Extracting {member_stripped}...\")\n",
    "                z.extract(member, DATA_PARENT_DIR)\n",
    "\n",
    "                # If the extracted filename has whitespace, rename it\n",
    "                extracted_raw = os.path.join(DATA_PARENT_DIR, member)\n",
    "                extracted = os.path.join(DATA_PARENT_DIR, member_stripped)\n",
    "                \n",
    "                if extracted_raw != extracted and os.path.exists(extracted_raw):\n",
    "                    print(f\"Renaming extracted file (removing whitespace)...\")\n",
    "                    os.rename(extracted_raw, extracted)\n",
    "\n",
    "                # Wait loop: verify file exists and size is stable (not growing)\n",
    "                print(f\"Waiting for extraction to complete...\")\n",
    "                max_wait = 120  # seconds\n",
    "                last_size = 0\n",
    "                stable_count = 0\n",
    "                \n",
    "                for i in range(max_wait):\n",
    "                    if os.path.exists(extracted):\n",
    "                        try:\n",
    "                            current_size = os.path.getsize(extracted)\n",
    "                            if current_size > 0:\n",
    "                                if current_size == last_size:\n",
    "                                    stable_count += 1\n",
    "                                    if stable_count >= 3:  # File size unchanged for 3 seconds = stable\n",
    "                                        print(f\"File extraction complete. Size: {current_size} bytes\")\n",
    "                                        break\n",
    "                                else:\n",
    "                                    stable_count = 0\n",
    "                                last_size = current_size\n",
    "                        except OSError:\n",
    "                            pass  # File may be locked during extraction\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                # Additional wait to ensure all buffers are flushed to disk\n",
    "                print(f\"Flushing disk buffers...\")\n",
    "                time.sleep(2)\n",
    "\n",
    "                # Move to final destination if needed\n",
    "                os.makedirs(os.path.dirname(xpt_path), exist_ok=True)\n",
    "                \n",
    "                if os.path.abspath(extracted) != os.path.abspath(xpt_path):\n",
    "                    print(f\"Moving extracted file to {xpt_path}\")\n",
    "                    try:\n",
    "                        shutil.move(extracted, xpt_path)\n",
    "                    except Exception as move_err:\n",
    "                        print(f\"shutil.move failed, using os.replace: {move_err}\")\n",
    "                        if os.path.exists(xpt_path):\n",
    "                            os.remove(xpt_path)\n",
    "                        os.replace(extracted, xpt_path)\n",
    "        except zipfile.BadZipFile as e:\n",
    "            raise RuntimeError(f\"Bad zip file for {year}: {e}\")\n",
    "    else:\n",
    "        print(f\"[CACHED] {year} xpt\")\n",
    "\n",
    "    return xpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ecb2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_value(canonical: str, year: int, val) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Decode a single scalar value to its human-readable label.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    canonical : str\n",
    "        Canonical field name (key in VALUE_MAP / VALUE_TEXT_MAP).\n",
    "    year : int\n",
    "        Year to use for per-year mapping.\n",
    "    val : scalar\n",
    "        Value to decode (int, str convertible to int, or missing).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or pd.NA\n",
    "        Mapped label string, or pd.NA if the value is missing/unmapped.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Missing values (pd.isna) or non-integer-convertible values return pd.NA.\n",
    "    - First attempts to use the per-year mapping in VALUE_MAP; if not found,\n",
    "      falls back to the constant mapping in VALUE_TEXT_MAP.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return pd.NA\n",
    "    try:\n",
    "        key = int(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return pd.NA\n",
    "\n",
    "    # Try per-year mapping first\n",
    "    per_year_map = VALUE_MAP.get(canonical, {}).get(year, {})\n",
    "    label = per_year_map.get(key)\n",
    "    if label is not None:\n",
    "        return label\n",
    "\n",
    "    # Fallback to constant mapping\n",
    "    label = VALUE_TEXT_MAP.get(canonical, {}).get(key)\n",
    "    return label if label is not None else pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "043fd0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_series(canonical: str, year: int, series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Decodes a single scalar value into its human-readable label using a hierarchical lookup.\n",
    "\n",
    "    This function first attempts to convert the input `val` to an integer. It then resolves \n",
    "    the label by checking sources in the following order:\n",
    "    1.  **Year-Specific:** Checks `VALUE_MAP` for a mapping specific to the provided `year`.\n",
    "    2.  **Global Fallback:** If no year-specific match is found, checks `VALUE_TEXT_MAP` \n",
    "        for a constant/global mapping.\n",
    "\n",
    "    Args:\n",
    "        canonical (str): The canonical field name (the key used in the configuration maps).\n",
    "        year (int): The survey year associated with the value (used for versioned lookups).\n",
    "        val (Any): The scalar value to decode. This handles integers, strings that can be \n",
    "            converted to integers, and missing values.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The decoded text label. Returns `pd.NA` in the following cases:\n",
    "            - The input `val` is missing (`pd.isna`).\n",
    "            - The input `val` cannot be converted to an integer.\n",
    "            - No mapping exists for the code in either the year-specific or global maps.\n",
    "    \"\"\"\n",
    "    if series is None:\n",
    "        return pd.Series(dtype=\"object\")\n",
    "    if series.empty:\n",
    "        return pd.Series(index=series.index, dtype=\"object\")\n",
    "\n",
    "    # coerce to integer codes where possible (nullable Int64)\n",
    "    codes = pd.to_numeric(series, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    per_year_map = VALUE_MAP.get(canonical, {}).get(year, {})\n",
    "    fallback_map = VALUE_TEXT_MAP.get(canonical, {})\n",
    "    # fallback_map provides defaults, per_year_map overrides them\n",
    "    combined_map = {**fallback_map, **per_year_map}\n",
    "\n",
    "    mapped = codes.map(combined_map)\n",
    "\n",
    "    # preserve explicit missing codes and ensure unmapped numeric codes become pd.NA\n",
    "    mapped = mapped.where(~codes.isna(), pd.NA)\n",
    "    mapped = mapped.where(mapped.notna(), pd.NA)\n",
    "\n",
    "    return mapped.astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71005b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_days(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normalizes a Series of day-count values to the 2016 categorical coding standard.\n",
    "\n",
    "    This transformation simplifies continuous day-count fields (and their specific \n",
    "    \"None\" codes like 88) into a discrete 4-level scale. Input values are first \n",
    "    coerced to numeric types.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The input Series containing raw day-count data. \n",
    "            Handles numeric types or strings convertible to numbers.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series of 'Int64' dtype containing the normalized codes:\n",
    "            * **1**: Zero days (mapped from raw values `0` or `88`).\n",
    "            * **2**: 1-13 days (mapped from range `1-13`).\n",
    "            * **3**: 14+ days (mapped from range `14-96`).\n",
    "            * **9**: Refused/Unknown (mapped from `9` or `99`).\n",
    "            * **<NA>**: Any other value, parsing error, or empty input.\n",
    "    \"\"\"\n",
    "    if series is None or series.empty:\n",
    "        return pd.Series(dtype=\"Int64\")\n",
    "    nums = pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "    def _norm(v):\n",
    "        if pd.isna(v):\n",
    "            return pd.NA\n",
    "        vi = int(v)\n",
    "        if vi in (9, 99):\n",
    "            return 9\n",
    "        if vi in (88, 0):\n",
    "            return 1\n",
    "        if 1 <= vi <= 13:\n",
    "            return 2\n",
    "        if vi >= 14 and vi < 97:\n",
    "            return 3\n",
    "        return pd.NA\n",
    "\n",
    "    normalized = nums.apply(lambda x: _norm(x)).astype(\"Int64\")\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc6b72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_year_mappings(df_raw: pd.DataFrame, year: int) -> list:\n",
    "    \"\"\"\n",
    "    Identifies canonical fields defined in VAR_MAP that are missing from the raw DataFrame.\n",
    "\n",
    "    This function iterates through the global `VAR_MAP` configuration to find expected \n",
    "    column names for the specified `year`. It supports mapping definitions where the \n",
    "    target can be a single string or a list of candidate strings. A field is considered \n",
    "    \"found\" if at least one of its candidate columns exists in `df_raw`.\n",
    "\n",
    "    Args:\n",
    "        df_raw (pd.DataFrame): The raw DataFrame loaded from the BRFSS XPT file for the given year.\n",
    "        year (int): The specific year being validated (used to look up expected columns in VAR_MAP).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of canonical field names (strings) that could not be resolved to any \n",
    "        column in `df_raw`.\n",
    "\n",
    "    Notes:\n",
    "        - Prints a summary log to stdout indicating how many fields are missing.\n",
    "        - If the mapping for a year is `None` or invalid, the field is treated as missing.\n",
    "    \"\"\"\n",
    "    missing = []\n",
    "    for canonical, year_fields in VAR_MAP.items():\n",
    "        candidates = None\n",
    "        if isinstance(year_fields, dict):\n",
    "            candidates = year_fields.get(year)\n",
    "        if candidates is None:\n",
    "            candidates = []\n",
    "        elif isinstance(candidates, str):\n",
    "            candidates = [candidates]\n",
    "        elif not isinstance(candidates, (list, tuple, set)):\n",
    "            try:\n",
    "                candidates = list(candidates)\n",
    "            except Exception:\n",
    "                candidates = []\n",
    "\n",
    "        found = any(c in df_raw.columns for c in candidates)\n",
    "        if not found:\n",
    "            missing.append(canonical)\n",
    "\n",
    "    print(f\"Year {year}: {len(missing)} of {len(VAR_MAP)} canonical fields missing\")\n",
    "    if missing:\n",
    "        print(f\"Missing fields for {year}: {missing}\")\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "685da329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map canonical field names to specific normalization functions\n",
    "SPECIAL_TRANSFORMS: Dict[str, Callable[[pd.Series], pd.Series]] = {\n",
    "    \"PHYSICAL_HEALTH_STATUS\": normalize_days,\n",
    "    \"MENTAL_HEALTH_STATUS\": normalize_days\n",
    "}\n",
    "\n",
    "def load_year(year: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads, maps, and normalizes a single year of BRFSS survey data.\n",
    "\n",
    "    This function orchestrates the data loading pipeline for a specific year:\n",
    "    1.  **Fetch:** Ensures the local SAS XPT file exists (via `ensure_xpt`).\n",
    "    2.  **Load:** Reads the XPT file into a pandas DataFrame using 'latin1' encoding.\n",
    "    3.  **Map:** Renames raw columns to canonical names using `VAR_MAP`. It handles\n",
    "        cases where multiple candidate columns exist by picking the first match.\n",
    "    4.  **Normalize:** Applies special transformation logic (defined in `SPECIAL_TRANSFORMS`)\n",
    "        to specific fields (e.g., 'PHYSICAL_HEALTH_STATUS').\n",
    "\n",
    "    Args:\n",
    "        year (int): The 4-digit year to load (e.g., 2019).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A normalized DataFrame containing only the canonical columns defined \n",
    "        in `VAR_MAP`, plus a 'YEAR' column. Returns an empty DataFrame if the XPT file cannot be read.\n",
    "\n",
    "    Notes:\n",
    "        - Logs warnings if canonical columns are missing from the raw data.\n",
    "        - Fills missing columns with `np.nan`.\n",
    "    \"\"\"\n",
    "    xpt_path = ensure_xpt(year)\n",
    "    try:\n",
    "        # 'latin1' is required for SAS XPT files\n",
    "        df_raw = pd.read_sas(xpt_path, format=\"xport\", encoding=\"latin1\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read SAS XPT for year {year}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    validate_year_mappings(df_raw, year)\n",
    "    \n",
    "    out = pd.DataFrame()\n",
    "    out[\"YEAR\"] = year\n",
    "\n",
    "    for canonical, year_fields in VAR_MAP.items():\n",
    "        # Resolve candidates (handle string vs list vs dict lookup)\n",
    "        candidates = year_fields.get(year, [])\n",
    "        if isinstance(candidates, str):\n",
    "            candidates = [candidates]\n",
    "        \n",
    "        # Find the first matching column in the raw data\n",
    "        raw_col = next((df_raw[c] for c in candidates if c in df_raw.columns), None)\n",
    "\n",
    "        if raw_col is None:\n",
    "            logger.warning(f\"Year {year}: Missing column for '{canonical}' (checked: {candidates})\")\n",
    "            out[canonical] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Apply transformations if defined, otherwise pass through\n",
    "        if canonical in SPECIAL_TRANSFORMS:\n",
    "            try:\n",
    "                out[canonical] = SPECIAL_TRANSFORMS[canonical](raw_col)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Normalization failed for {canonical} in {year}: {e}\")\n",
    "                out[canonical] = raw_col\n",
    "        else:\n",
    "            out[canonical] = raw_col\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3827b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_memory_bytes() -> int:\n",
    "    \"\"\"\n",
    "    Returns the current memory usage of the process in bytes.\n",
    "\n",
    "    This function prioritizes using `psutil` to retrieve the Resident Set Size (RSS), \n",
    "    which represents the portion of memory occupied by the process that is held in \n",
    "    main memory (RAM). If `psutil` is unavailable or fails, it falls back to \n",
    "    `tracemalloc` to report the current size of memory blocks traced by Python.\n",
    "\n",
    "    Returns:\n",
    "        int: The memory usage in bytes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        proc = psutil.Process(os.getpid())\n",
    "        return int(proc.memory_info().rss)\n",
    "    except Exception:\n",
    "        # Ensure tracemalloc is started\n",
    "        if not tracemalloc.is_tracing():\n",
    "            tracemalloc.start()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        return int(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd932c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_bytes(b: int) -> str:\n",
    "    \"\"\"\n",
    "    Formats a byte count into a human-readable string with appropriate units (B, KB, MB, etc.).\n",
    "\n",
    "    This function calculates the most suitable unit based on a binary prefix (1024 base).\n",
    "    It handles negative values by preserving the sign and treats `None` as 0 bytes.\n",
    "    The result is formatted to two decimal places.\n",
    "\n",
    "    Args:\n",
    "        b (int): The number of bytes to format. Can be negative or None.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representing the size (e.g., \"1.23 MB\", \"-500.00 B\").\n",
    "             Returns \"0 B\" if the input is None.\n",
    "    \"\"\"\n",
    "    if b is None:\n",
    "        return \"0 B\"\n",
    "    sign = \"\" if b >= 0 else \"-\"\n",
    "    b = abs(int(b))\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if b < 1024:\n",
    "            return f\"{sign}{b:.2f} {unit}\"\n",
    "        b /= 1024\n",
    "    return f\"{sign}{b:.2f} PB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ee06b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multi_year(years: list, output_dir: Path = PROCESSED_DATA_DIR) -> None:\n",
    "    \"\"\"\n",
    "    Iteratively loads, normalizes, and exports individual yearly BRFSS datasets.\n",
    "\n",
    "    This function processes a list of years sequentially. Its sole responsibility is \n",
    "    generating the standardized `brfss_{year}.csv` files. It does *not* perform \n",
    "    any merging.\n",
    "\n",
    "    For each year, it:\n",
    "    1.  **Loads** the raw SAS data using `load_year`.\n",
    "    2.  **Aligns** columns to a strict canonical order defined by `VAR_MAP`.\n",
    "    3.  **Exports** the normalized data to a CSV file (e.g., `brfss_2019.csv`).\n",
    "    4.  **Cleans up** memory immediately after writing.\n",
    "\n",
    "    Args:\n",
    "        years (list): A list of integer years to process (e.g., `[2019, 2020]`).\n",
    "        output_dir (Path, optional): The directory path where output files will be saved. \n",
    "            Defaults to `PROCESSED_DATA_DIR`.\n",
    "\n",
    "    Returns:\n",
    "        None: The output is side-effect based (CSV files written to disk).\n",
    "    \"\"\"\n",
    "    # Ensure consistent column order\n",
    "    canonical_cols = [\"YEAR\"] + list(VAR_MAP.keys())\n",
    "    \n",
    "    # Use tqdm for a professional progress bar\n",
    "    pbar = tqdm(years, desc=\"Generating Yearly CSVs\")\n",
    "    \n",
    "    for year in pbar:\n",
    "        mem_before = _get_memory_bytes()\n",
    "        pbar.set_postfix_str(f\"Mem: {_format_bytes(mem_before)}\")\n",
    "        \n",
    "        df_year = load_year(year)\n",
    "        \n",
    "        if df_year.empty:\n",
    "            logger.warning(f\"Year {year}: No data loaded, skipping export.\")\n",
    "            continue\n",
    "\n",
    "        # Align columns to ensure schema consistency\n",
    "        df_year = df_year.reindex(columns=canonical_cols)\n",
    "        \n",
    "        # Write individual year CSV\n",
    "        year_csv_path = output_dir / f\"brfss_{year}.csv\"\n",
    "        df_year.to_csv(year_csv_path, index=False)\n",
    "        logger.info(f\"Saved {year} data to {year_csv_path.name} ({len(df_year)} rows)\")\n",
    "\n",
    "        # Explicit Memory Cleanup\n",
    "        del df_year\n",
    "        gc.collect()\n",
    "        \n",
    "        mem_after = _get_memory_bytes()\n",
    "        logger.debug(f\"Year {year} cleanup. Delta: {_format_bytes(mem_after - mem_before)}\")\n",
    "\n",
    "    logger.info(\"Batch processing complete. Individual CSVs generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ff7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_yearly_datasets(data_dir: Path, start_year: int, end_year: int, output_filename: str = \"BRFSS_merged.csv\") -> str:\n",
    "    \"\"\"\n",
    "    Consolidates existing yearly CSV files into a single merged dataset.\n",
    "\n",
    "    This method scans the specified directory for files matching the pattern \n",
    "    `brfss_{year}.csv` within the inclusive range [`start_year`, `end_year`].\n",
    "    It reads each file and appends it to a unified CSV, ensuring the header is \n",
    "    written only once.\n",
    "\n",
    "    Args:\n",
    "        data_dir (Path): The directory containing the yearly CSV files.\n",
    "        start_year (int): The starting year of the range (inclusive).\n",
    "        end_year (int): The ending year of the range (inclusive).\n",
    "        output_filename (str, optional): The name of the final merged CSV file. \n",
    "            Defaults to \"BRFSS_merged.csv\".\n",
    "\n",
    "    Returns:\n",
    "        str: The full string path to the merged CSV file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If a specific year's CSV file (e.g., `brfss_2015.csv`) \n",
    "            is missing from `data_dir`.\n",
    "    \"\"\"\n",
    "    merged_path = data_dir / output_filename\n",
    "    years_to_merge = list(range(start_year, end_year + 1))\n",
    "    \n",
    "    total_rows = 0\n",
    "    first_batch = True\n",
    "    \n",
    "    logger.info(f\"Starting merge for years {start_year}-{end_year} into {output_filename}\")\n",
    "    \n",
    "    # Progress bar for the merging phase\n",
    "    pbar = tqdm(years_to_merge, desc=\"Merging Datasets\")\n",
    "    \n",
    "    for year in pbar:\n",
    "        source_csv = data_dir / f\"brfss_{year}.csv\"\n",
    "        \n",
    "        if not source_csv.exists():\n",
    "            logger.warning(f\"Source file missing for year {year}: {source_csv}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Read the previously generated CSV\n",
    "        # We assume these files are already normalized and clean from `load_multi_year`\n",
    "        try:\n",
    "            df_chunk = pd.read_csv(source_csv)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read {source_csv}: {e}\")\n",
    "            continue\n",
    "\n",
    "        row_count = len(df_chunk)\n",
    "        pbar.set_postfix_str(f\"Appending {year} ({row_count} rows)\")\n",
    "\n",
    "        # Append to the consolidated file\n",
    "        # 'w' mode for the first file (overwrites existing), 'a' mode for subsequent\n",
    "        write_mode = \"w\" if first_batch else \"a\"\n",
    "        write_header = first_batch\n",
    "        \n",
    "        df_chunk.to_csv(merged_path, mode=write_mode, header=write_header, index=False)\n",
    "        \n",
    "        total_rows += row_count\n",
    "        first_batch = False\n",
    "        \n",
    "        # Immediate cleanup to keep memory footprint low during merge\n",
    "        del df_chunk\n",
    "        gc.collect()\n",
    "\n",
    "    logger.info(f\"Merge Complete. Written {total_rows} rows to {merged_path}\")\n",
    "    return str(merged_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b03353e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94857514ae141e0b628969adc320268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Yearly CSVs:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2015\n",
      "[UNZIP] 2015\n",
      "Extracting LLCP2015.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1165490800 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2015: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:13:58 - INFO - Saved 2015 data to brfss_2015.csv (441456 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2016\n",
      "[UNZIP] 2016\n",
      "Extracting LLCP2016.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1067474400 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2016: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:14:53 - INFO - Saved 2016 data to brfss_2016.csv (486303 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2017\n",
      "[UNZIP] 2017\n",
      "Extracting LLCP2017.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1288446720 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2017: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:16:10 - INFO - Saved 2017 data to brfss_2017.csv (450016 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2018\n",
      "[UNZIP] 2018\n",
      "Extracting LLCP2018.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 961961120 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2018: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:16:58 - INFO - Saved 2018 data to brfss_2018.csv (437436 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2019\n",
      "[UNZIP] 2019\n",
      "Extracting LLCP2019.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1136901120 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2019: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:17:43 - INFO - Saved 2019 data to brfss_2019.csv (418268 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2020\n",
      "[UNZIP] 2020\n",
      "Extracting LLCP2020.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 889974880 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2020: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:18:19 - INFO - Saved 2020 data to brfss_2020.csv (401958 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2021\n",
      "[UNZIP] 2021\n",
      "Extracting LLCP2021.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1055538560 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2021: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:19:08 - INFO - Saved 2021 data to brfss_2021.csv (438693 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2022\n",
      "[UNZIP] 2022\n",
      "Extracting LLCP2022.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1160060640 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2022: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:19:59 - INFO - Saved 2022 data to brfss_2022.csv (445132 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2023\n",
      "[UNZIP] 2023\n",
      "Extracting LLCP2023.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1205554400 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2023: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:20:47 - INFO - Saved 2023 data to brfss_2023.csv (433323 rows)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DOWNLOAD] 2024\n",
      "[UNZIP] 2024\n",
      "Extracting LLCP2024.XPT...\n",
      "Renaming extracted file (removing whitespace)...\n",
      "Waiting for extraction to complete...\n",
      "File extraction complete. Size: 1093874240 bytes\n",
      "Flushing disk buffers...\n",
      "Year 2024: 0 of 20 canonical fields missing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:21:35 - INFO - Saved 2024 data to brfss_2024.csv (457670 rows)\n",
      "20:21:35 - INFO - Batch processing complete. Individual CSVs generated.\n"
     ]
    }
   ],
   "source": [
    "start_year = 2015\n",
    "end_year = 2024\n",
    "merged_output_file_name = f\"BRFSS_{start_year}_{end_year}.csv\"\n",
    "years = list(range(start_year, end_year+1))\n",
    "output_dir = PROCESSED_DATA_DIR\n",
    "outputfile = os.path.join(output_dir, merged_output_file_name)\n",
    "\n",
    "load_multi_year(years, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9eeb0eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:40:30 - INFO - Starting merge for years 2015-2024 into BRFSS_2015_2024.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a385133509e4fd1b40fc2f3cf9fd52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Merging Datasets:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:41:04 - INFO - Merge Complete. Written 4410255 rows to c:\\github\\brfss-diabetes-trends\\data_processed\\BRFSS_2015_2024.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping c:\\github\\brfss-diabetes-trends\\data_processed\\BRFSS_2015_2024.csv...\n",
      "Created c:\\github\\brfss-diabetes-trends\\data_processed\\BRFSS_2015_2024.zip\n"
     ]
    }
   ],
   "source": [
    "merge_yearly_datasets(output_dir, start_year, end_year, merged_output_file_name)\n",
    "\n",
    "# us os pth instead of raw string for better compatibility\n",
    "csv_path = os.path.join(output_dir, merged_output_file_name)\n",
    "zip_path = os.path.join(output_dir, f\"{merged_output_file_name[:-4]}.zip\")\n",
    "\n",
    "print(f\"Zipping {csv_path}...\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(csv_path, arcname=merged_output_file_name)\n",
    "print(f\"Created {zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_gl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
