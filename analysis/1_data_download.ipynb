{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615d6bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import gc\n",
    "import tracemalloc\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bd3653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "YEARS = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
    "\n",
    "DATA_DIR = os.path.join(os.getcwd(), \"..\", \"data_raw\")\n",
    "PROCESSED_DATA_DIR = os.path.join(os.getcwd(), \"..\", \"data_processed\")\n",
    "CONFIG_DIR = os.path.join(os.getcwd(), \"..\", \"config\")\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)\n",
    "\n",
    "BASE_URL = \"https://www.cdc.gov/brfss/annual_data/{year}/files/LLCP{year}XPT.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "82539bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] Loading configuration from: c:\\github\\brfss-diabetes-trends\\analysis\\..\\config\\VAR_MAP.json\n",
      "[CONFIG] - Loaded 20 canonical fields from VAR_MAP.json\n",
      "[CONFIG] Loading configuration from: c:\\github\\brfss-diabetes-trends\\analysis\\..\\config\\VALUE_MAP.json\n",
      "[CONFIG] - Loaded 20 canonical fields from VALUE_MAP.json\n",
      "[CONFIG] Loading configuration from: c:\\github\\brfss-diabetes-trends\\analysis\\..\\config\\VALUE_TEXT_MAP.json\n",
      "[CONFIG] - Loaded 20 canonical fields from VALUE_TEXT_MAP.json\n",
      "[CONFIG] Mappings loaded successfully.\n",
      "[CONFIG] - VAR_MAP: 20 fields\n",
      "[CONFIG] - VALUE_MAP: 20 fields\n",
      "[CONFIG] - VALUE_TEXT_MAP: 20 fields\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate_mappings(config_dir: str, config_file_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads and validates a specific mapping configuration file from JSON.\n",
    "\n",
    "    This function reads a JSON file from the specified directory and parses it \n",
    "    into a dictionary. The structure of the returned dictionary depends on the \n",
    "    `config_file_name` provided, with specific handling for 'VAR_MAP.json', \n",
    "    'VALUE_MAP.json', and 'VALUE_TEXT_MAP.json'. Keys representing years or \n",
    "    codes are converted to integers.\n",
    "\n",
    "    Args:\n",
    "        config_dir (str): The directory path where the configuration file is located.\n",
    "        config_file_name (str): The specific name of the JSON configuration file \n",
    "            (e.g., \"VAR_MAP.json\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the loaded configuration map with integer-converted keys.\n",
    "            - For VAR_MAP: {canonical_name: {year: column_name}}\n",
    "            - For VALUE_MAP: {canonical_name: {year: {code: value}}}\n",
    "            - For VALUE_TEXT_MAP: {canonical_name: {code: value}}\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified configuration file does not exist.\n",
    "        ValueError: If the `config_file_name` is not one of the expected known types.\n",
    "    \"\"\"\n",
    "    config_file_path = os.path.join(config_dir, config_file_name)\n",
    "    if not os.path.exists(config_file_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {config_file_path}\")\n",
    "    \n",
    "    print(f\"[CONFIG] Loading configuration from: {config_file_path}\")\n",
    "    with open(config_file_path, 'r') as f:\n",
    "        config_data = json.load(f)\n",
    "\n",
    "    config_map = {}\n",
    "    if config_file_name == \"VAR_MAP.json\":\n",
    "        for canonical, year_fields in config_data.items():\n",
    "            config_map[canonical] = {int(year): col for year, col in year_fields.items()}\n",
    "    elif config_file_name == \"VALUE_MAP.json\":\n",
    "        for canonical, year_values in config_data.items():\n",
    "            config_map[canonical] = {}\n",
    "            for year, code_map in year_values.items():\n",
    "                config_map[canonical][int(year)] = {int(k): v for k, v in code_map.items()}\n",
    "    elif config_file_name == \"VALUE_TEXT_MAP.json\":\n",
    "        for canonical, code_map in config_data.items():\n",
    "            config_map[canonical] = {int(k): v for k, v in code_map.items()}\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown configuration file: {config_file_name}\")\n",
    "\n",
    "    print(f\"[CONFIG] - Loaded {len(config_map)} canonical fields from {config_file_name}\")\n",
    "    return config_map\n",
    "\n",
    "VAR_MAP = load_and_validate_mappings(CONFIG_DIR, \"VAR_MAP.json\")\n",
    "VALUE_MAP = load_and_validate_mappings(CONFIG_DIR, \"VALUE_MAP.json\")\n",
    "VALUE_TEXT_MAP = load_and_validate_mappings(CONFIG_DIR, \"VALUE_TEXT_MAP.json\")\n",
    "\n",
    "print(f\"[CONFIG] Mappings loaded successfully.\")\n",
    "print(f\"[CONFIG] - VAR_MAP: {len(VAR_MAP)} fields\")\n",
    "print(f\"[CONFIG] - VALUE_MAP: {len(VALUE_MAP)} fields\")\n",
    "print(f\"[CONFIG] - VALUE_TEXT_MAP: {len(VALUE_TEXT_MAP)} fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_xpt(year, retries: int = 3, timeout: int = 30):\n",
    "    \"\"\"\n",
    "    Ensures that the LLCP{year}.XPT file exists locally, downloading and extracting it if necessary.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1.  **Download:** If the source ZIP file is missing, it is downloaded from `BASE_URL` \n",
    "        using exponential backoff for retries.\n",
    "    2.  **Extraction:** If the XPT file is missing, the function locates the first `.xpt` \n",
    "        file within the ZIP archive (case-insensitive).\n",
    "    3.  **Stabilization:** It extracts the file, handles potential whitespace in the \n",
    "        filename, and waits for the file size to stabilize (ensuring extraction is \n",
    "        complete) before moving it to the final destination.\n",
    "\n",
    "    Args:\n",
    "        year (int): The target year to fetch (e.g., 2019).\n",
    "        retries (int, optional): The maximum number of download attempts. Defaults to 3.\n",
    "        timeout (int, optional): The request timeout in seconds for each attempt. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        str: The full filesystem path to the local .XPT file.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the ZIP archive does not contain any .xpt files.\n",
    "        zipfile.BadZipFile: If the local ZIP file is corrupt or invalid.\n",
    "        requests.exceptions.RequestException: If the download fails after all retries are exhausted.\n",
    "    \"\"\"\n",
    "    DATA_PARENT_DIR = os.path.join(DATA_DIR, f\"{year}\")\n",
    "    if not os.path.exists(DATA_PARENT_DIR):\n",
    "        os.makedirs(DATA_PARENT_DIR, exist_ok=True)\n",
    "    zip_path = os.path.join(DATA_PARENT_DIR, f\"LLCP{year}.zip\")\n",
    "    xpt_path = os.path.join(DATA_PARENT_DIR, f\"LLCP{year}.XPT\")\n",
    "\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(f\"[DOWNLOAD] {year}\")\n",
    "        url = BASE_URL.format(year=year)\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "                    r.raise_for_status()\n",
    "                    with open(zip_path, \"wb\") as f:\n",
    "                        for chunk in r.iter_content(chunk_size=8192):\n",
    "                            if chunk:\n",
    "                                f.write(chunk)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Download attempt {attempt} failed for {year}: {e}\")\n",
    "                if attempt < retries:\n",
    "                    time.sleep(2 ** attempt)\n",
    "                else:\n",
    "                    raise\n",
    "    else:\n",
    "        print(f\"[CACHED] {year} zip\")\n",
    "\n",
    "    if not os.path.exists(xpt_path):\n",
    "        print(f\"[UNZIP] {year}\")\n",
    "        try:\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "                # Find the .xpt file (case insensitive, strip whitespace)\n",
    "                all_files = z.namelist()\n",
    "                members = [m for m in all_files if m.lower().strip().endswith(\".xpt\")]\n",
    "                if not members:\n",
    "                    print(f\"DEBUG: Contents of zip {year}: {all_files}\")\n",
    "                    raise RuntimeError(f\"No .xpt found in zip for {year}\")\n",
    "                \n",
    "                # extract first .xpt and move it to expected path\n",
    "                member = members[0]\n",
    "                member_stripped = member.strip()  # Remove leading/trailing whitespace\n",
    "                print(f\"Extracting {member_stripped}...\")\n",
    "                z.extract(member, DATA_PARENT_DIR)\n",
    "\n",
    "                # If the extracted filename has whitespace, rename it\n",
    "                extracted_raw = os.path.join(DATA_PARENT_DIR, member)\n",
    "                extracted = os.path.join(DATA_PARENT_DIR, member_stripped)\n",
    "                \n",
    "                if extracted_raw != extracted and os.path.exists(extracted_raw):\n",
    "                    print(f\"Renaming extracted file (removing whitespace)...\")\n",
    "                    os.rename(extracted_raw, extracted)\n",
    "\n",
    "                # Wait loop: verify file exists and size is stable (not growing)\n",
    "                print(f\"Waiting for extraction to complete...\")\n",
    "                max_wait = 120  # seconds\n",
    "                last_size = 0\n",
    "                stable_count = 0\n",
    "                \n",
    "                for i in range(max_wait):\n",
    "                    if os.path.exists(extracted):\n",
    "                        try:\n",
    "                            current_size = os.path.getsize(extracted)\n",
    "                            if current_size > 0:\n",
    "                                if current_size == last_size:\n",
    "                                    stable_count += 1\n",
    "                                    if stable_count >= 3:  # File size unchanged for 3 seconds = stable\n",
    "                                        print(f\"File extraction complete. Size: {current_size} bytes\")\n",
    "                                        break\n",
    "                                else:\n",
    "                                    stable_count = 0\n",
    "                                last_size = current_size\n",
    "                        except OSError:\n",
    "                            pass  # File may be locked during extraction\n",
    "                    time.sleep(1)\n",
    "                \n",
    "                # Additional wait to ensure all buffers are flushed to disk\n",
    "                print(f\"Flushing disk buffers...\")\n",
    "                time.sleep(2)\n",
    "\n",
    "                # Move to final destination if needed\n",
    "                os.makedirs(os.path.dirname(xpt_path), exist_ok=True)\n",
    "                \n",
    "                if os.path.abspath(extracted) != os.path.abspath(xpt_path):\n",
    "                    print(f\"Moving extracted file to {xpt_path}\")\n",
    "                    try:\n",
    "                        shutil.move(extracted, xpt_path)\n",
    "                    except Exception as move_err:\n",
    "                        print(f\"shutil.move failed, using os.replace: {move_err}\")\n",
    "                        if os.path.exists(xpt_path):\n",
    "                            os.remove(xpt_path)\n",
    "                        os.replace(extracted, xpt_path)\n",
    "        except zipfile.BadZipFile as e:\n",
    "            raise RuntimeError(f\"Bad zip file for {year}: {e}\")\n",
    "    else:\n",
    "        print(f\"[CACHED] {year} xpt\")\n",
    "\n",
    "    return xpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ecb2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_value(canonical: str, year: int, val) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Decode a single scalar value to its human-readable label.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    canonical : str\n",
    "        Canonical field name (key in VALUE_MAP / VALUE_TEXT_MAP).\n",
    "    year : int\n",
    "        Year to use for per-year mapping.\n",
    "    val : scalar\n",
    "        Value to decode (int, str convertible to int, or missing).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or pd.NA\n",
    "        Mapped label string, or pd.NA if the value is missing/unmapped.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Missing values (pd.isna) or non-integer-convertible values return pd.NA.\n",
    "    - First attempts to use the per-year mapping in VALUE_MAP; if not found,\n",
    "      falls back to the constant mapping in VALUE_TEXT_MAP.\n",
    "    \"\"\"\n",
    "    if pd.isna(val):\n",
    "        return pd.NA\n",
    "    try:\n",
    "        key = int(val)\n",
    "    except (ValueError, TypeError):\n",
    "        return pd.NA\n",
    "\n",
    "    # Try per-year mapping first\n",
    "    per_year_map = VALUE_MAP.get(canonical, {}).get(year, {})\n",
    "    label = per_year_map.get(key)\n",
    "    if label is not None:\n",
    "        return label\n",
    "\n",
    "    # Fallback to constant mapping\n",
    "    label = VALUE_TEXT_MAP.get(canonical, {}).get(key)\n",
    "    return label if label is not None else pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fd0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_series(canonical: str, year: int, series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Decodes a single scalar value into its human-readable label using a hierarchical lookup.\n",
    "\n",
    "    This function first attempts to convert the input `val` to an integer. It then resolves \n",
    "    the label by checking sources in the following order:\n",
    "    1.  **Year-Specific:** Checks `VALUE_MAP` for a mapping specific to the provided `year`.\n",
    "    2.  **Global Fallback:** If no year-specific match is found, checks `VALUE_TEXT_MAP` \n",
    "        for a constant/global mapping.\n",
    "\n",
    "    Args:\n",
    "        canonical (str): The canonical field name (the key used in the configuration maps).\n",
    "        year (int): The survey year associated with the value (used for versioned lookups).\n",
    "        val (Any): The scalar value to decode. This handles integers, strings that can be \n",
    "            converted to integers, and missing values.\n",
    "\n",
    "    Returns:\n",
    "        Optional[str]: The decoded text label. Returns `pd.NA` in the following cases:\n",
    "            - The input `val` is missing (`pd.isna`).\n",
    "            - The input `val` cannot be converted to an integer.\n",
    "            - No mapping exists for the code in either the year-specific or global maps.\n",
    "    \"\"\"\n",
    "    if series is None:\n",
    "        return pd.Series(dtype=\"object\")\n",
    "    if series.empty:\n",
    "        return pd.Series(index=series.index, dtype=\"object\")\n",
    "\n",
    "    # coerce to integer codes where possible (nullable Int64)\n",
    "    codes = pd.to_numeric(series, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    per_year_map = VALUE_MAP.get(canonical, {}).get(year, {})\n",
    "    fallback_map = VALUE_TEXT_MAP.get(canonical, {})\n",
    "    # fallback_map provides defaults, per_year_map overrides them\n",
    "    combined_map = {**fallback_map, **per_year_map}\n",
    "\n",
    "    mapped = codes.map(combined_map)\n",
    "\n",
    "    # preserve explicit missing codes and ensure unmapped numeric codes become pd.NA\n",
    "    mapped = mapped.where(~codes.isna(), pd.NA)\n",
    "    mapped = mapped.where(mapped.notna(), pd.NA)\n",
    "\n",
    "    return mapped.astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71005b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_days(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Normalizes a Series of day-count values to the 2016 categorical coding standard.\n",
    "\n",
    "    This transformation simplifies continuous day-count fields (and their specific \n",
    "    \"None\" codes like 88) into a discrete 4-level scale. Input values are first \n",
    "    coerced to numeric types.\n",
    "\n",
    "    Args:\n",
    "        series (pd.Series): The input Series containing raw day-count data. \n",
    "            Handles numeric types or strings convertible to numbers.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series of 'Int64' dtype containing the normalized codes:\n",
    "            * **1**: Zero days (mapped from raw values `0` or `88`).\n",
    "            * **2**: 1-13 days (mapped from range `1-13`).\n",
    "            * **3**: 14+ days (mapped from range `14-96`).\n",
    "            * **9**: Refused/Unknown (mapped from `9` or `99`).\n",
    "            * **<NA>**: Any other value, parsing error, or empty input.\n",
    "    \"\"\"\n",
    "    if series is None or series.empty:\n",
    "        return pd.Series(dtype=\"Int64\")\n",
    "    nums = pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "    def _norm(v):\n",
    "        if pd.isna(v):\n",
    "            return pd.NA\n",
    "        vi = int(v)\n",
    "        if vi in (9, 99):\n",
    "            return 9\n",
    "        if vi in (88, 0):\n",
    "            return 1\n",
    "        if 1 <= vi <= 13:\n",
    "            return 2\n",
    "        if vi >= 14 and vi < 97:\n",
    "            return 3\n",
    "        return pd.NA\n",
    "\n",
    "    normalized = nums.apply(lambda x: _norm(x)).astype(\"Int64\")\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_year_mappings(df_raw: pd.DataFrame, year: int) -> list:\n",
    "    \"\"\"\n",
    "    Identifies canonical fields defined in VAR_MAP that are missing from the raw DataFrame.\n",
    "\n",
    "    This function iterates through the global `VAR_MAP` configuration to find expected \n",
    "    column names for the specified `year`. It supports mapping definitions where the \n",
    "    target can be a single string or a list of candidate strings. A field is considered \n",
    "    \"found\" if at least one of its candidate columns exists in `df_raw`.\n",
    "\n",
    "    Args:\n",
    "        df_raw (pd.DataFrame): The raw DataFrame loaded from the BRFSS XPT file for the given year.\n",
    "        year (int): The specific year being validated (used to look up expected columns in VAR_MAP).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of canonical field names (strings) that could not be resolved to any \n",
    "        column in `df_raw`.\n",
    "\n",
    "    Notes:\n",
    "        - Prints a summary log to stdout indicating how many fields are missing.\n",
    "        - If the mapping for a year is `None` or invalid, the field is treated as missing.\n",
    "    \"\"\"\n",
    "    missing = []\n",
    "    for canonical, year_fields in VAR_MAP.items():\n",
    "        candidates = None\n",
    "        if isinstance(year_fields, dict):\n",
    "            candidates = year_fields.get(year)\n",
    "        if candidates is None:\n",
    "            candidates = []\n",
    "        elif isinstance(candidates, str):\n",
    "            candidates = [candidates]\n",
    "        elif not isinstance(candidates, (list, tuple, set)):\n",
    "            try:\n",
    "                candidates = list(candidates)\n",
    "            except Exception:\n",
    "                candidates = []\n",
    "\n",
    "        found = any(c in df_raw.columns for c in candidates)\n",
    "        if not found:\n",
    "            missing.append(canonical)\n",
    "\n",
    "    print(f\"Year {year}: {len(missing)} of {len(VAR_MAP)} canonical fields missing\")\n",
    "    if missing:\n",
    "        print(f\"Missing fields for {year}: {missing}\")\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3425bedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_year(year):\n",
    "    \"\"\"\n",
    "    Loads, maps, and normalizes a single year of BRFSS survey data.\n",
    "\n",
    "    This function orchestrates the data loading pipeline for a specific year:\n",
    "    1.  **Fetch:** Ensures the local SAS XPT file exists (downloading if necessary via `ensure_xpt`).\n",
    "    2.  **Load:** Reads the XPT file into a pandas DataFrame using 'latin1' encoding.\n",
    "    3.  **Validate:** Checks for missing canonical fields using `validate_year_mappings`.\n",
    "    4.  **Map & Normalize:** Iterates through `VAR_MAP` to rename columns to their canonical names.\n",
    "        - Prioritizes candidates in the order they appear in the configuration.\n",
    "        - Applies `normalize_days` logic to 'PHYSICAL_HEALTH_STATUS' and 'MENTAL_HEALTH_STATUS' fields.\n",
    "        - Fills missing columns with `np.nan`.\n",
    "\n",
    "    Args:\n",
    "        year (int): The 4-digit year to load (e.g., 2019).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A normalized DataFrame containing only the canonical columns defined \n",
    "        in `VAR_MAP`, plus a 'YEAR' column. Returns an empty DataFrame if the XPT file cannot be read.\n",
    "\n",
    "    Notes:\n",
    "        - Logs errors to stdout if specific columns are missing or if normalization fails.\n",
    "        - Uses `ensure_xpt` internally, which may trigger a file download.\n",
    "    \"\"\"\n",
    "    xpt = ensure_xpt(year)\n",
    "    try:\n",
    "        df_raw = pd.read_sas(xpt, format=\"xport\", encoding=\"latin1\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read SAS XPT for year {year}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # validate mappings and log a summary\n",
    "    validate_year_mappings(df_raw, year)\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "\n",
    "    for canonical, year_fields in VAR_MAP.items():\n",
    "        # normalize candidate(s) to a list\n",
    "        candidates = None\n",
    "        if isinstance(year_fields, dict):\n",
    "            candidates = year_fields.get(year)\n",
    "        if candidates is None:\n",
    "            candidates = []\n",
    "        elif isinstance(candidates, str):\n",
    "            candidates = [candidates]\n",
    "        elif not isinstance(candidates, (list, tuple, set)):\n",
    "            try:\n",
    "                candidates = list(candidates)\n",
    "            except Exception:\n",
    "                candidates = []\n",
    "\n",
    "        raw = None\n",
    "        for candidate in candidates:\n",
    "            if candidate in df_raw.columns:\n",
    "                raw = df_raw[candidate]\n",
    "                break\n",
    "\n",
    "        if raw is None:\n",
    "            print(f\"Year {year}: no column found for {canonical} among {candidates}\")\n",
    "            out[f\"{canonical}\"] = np.nan\n",
    "        else:\n",
    "            # Normalize day-count health fields to 2016 enum codes\n",
    "            if canonical in (\"PHYSICAL_HEALTH_STATUS\", \"MENTAL_HEALTH_STATUS\"):\n",
    "                try:\n",
    "                    normalized = normalize_days(raw)\n",
    "                    out[f\"{canonical}\"] = normalized\n",
    "                except Exception as e:\n",
    "                    print(f\"Normalization failed for {canonical} in {year}: {e}\")\n",
    "                    out[f\"{canonical}\"] = raw\n",
    "            else:\n",
    "                out[f\"{canonical}\"] = raw\n",
    "\n",
    "    out[\"YEAR\"] = year\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_memory_bytes() -> int:\n",
    "    \"\"\"\n",
    "    Returns the current memory usage of the process in bytes.\n",
    "\n",
    "    This function prioritizes using `psutil` to retrieve the Resident Set Size (RSS), \n",
    "    which represents the portion of memory occupied by the process that is held in \n",
    "    main memory (RAM). If `psutil` is unavailable or fails, it falls back to \n",
    "    `tracemalloc` to report the current size of memory blocks traced by Python.\n",
    "\n",
    "    Returns:\n",
    "        int: The memory usage in bytes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        proc = psutil.Process(os.getpid())\n",
    "        return int(proc.memory_info().rss)\n",
    "    except Exception:\n",
    "        # Ensure tracemalloc is started\n",
    "        if not tracemalloc.is_tracing():\n",
    "            tracemalloc.start()\n",
    "        current, peak = tracemalloc.get_traced_memory()\n",
    "        return int(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd932c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_bytes(b: int) -> str:\n",
    "    \"\"\"\n",
    "    Formats a byte count into a human-readable string with appropriate units (B, KB, MB, etc.).\n",
    "\n",
    "    This function calculates the most suitable unit based on a binary prefix (1024 base).\n",
    "    It handles negative values by preserving the sign and treats `None` as 0 bytes.\n",
    "    The result is formatted to two decimal places.\n",
    "\n",
    "    Args:\n",
    "        b (int): The number of bytes to format. Can be negative or None.\n",
    "\n",
    "    Returns:\n",
    "        str: A string representing the size (e.g., \"1.23 MB\", \"-500.00 B\").\n",
    "             Returns \"0 B\" if the input is None.\n",
    "    \"\"\"\n",
    "    if b is None:\n",
    "        return \"0 B\"\n",
    "    sign = \"\" if b >= 0 else \"-\"\n",
    "    b = abs(int(b))\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if b < 1024:\n",
    "            return f\"{sign}{b:.2f} {unit}\"\n",
    "        b /= 1024\n",
    "    return f\"{sign}{b:.2f} PB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0228e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multi_year(years: list, output_dir: str = \"data_processed\", merged_output_filename: str = \"BRFSS.csv\") -> str:\n",
    "    \"\"\"\n",
    "    Iteratively loads, normalizes, and exports BRFSS data for multiple years.\n",
    "\n",
    "    This function processes a list of years sequentially to manage memory usage. For each year:\n",
    "    1.  **Load & Normalize:** Calls `load_year` to fetch and clean the data.\n",
    "    2.  **Align Columns:** Reindexes the DataFrame to ensure a strict, consistent column order \n",
    "        defined by `VAR_MAP`, placing 'YEAR' first.\n",
    "    3.  **Export:**\n",
    "        - Writes an individual CSV file for the specific year (e.g., `brfss_2019.csv`).\n",
    "        - Appends the data to a cumulative merged CSV file (`merged_output_filename`).\n",
    "    4.  **Cleanup:** Explicitly deletes the DataFrame and triggers garbage collection \n",
    "        to free memory before processing the next year.\n",
    "\n",
    "    Args:\n",
    "        years (list): A list of integer years to process (e.g., `[2019, 2020]`).\n",
    "        output_dir (str, optional): The directory path for output files. Defaults to \"data_processed\".\n",
    "        merged_output_filename (str, optional): The filename for the cumulative CSV. \n",
    "            Defaults to \"BRFSS.csv\".\n",
    "\n",
    "    Returns:\n",
    "        str: The filename of the merged CSV file.\n",
    "\n",
    "    Notes:\n",
    "        - The function logs memory usage deltas before and after processing each year.\n",
    "        - The merged CSV is created in 'write' mode for the first year and 'append' mode \n",
    "          for subsequent years.\n",
    "    \"\"\"\n",
    "    canonical_cols = list(VAR_MAP.keys()) + [\"YEAR\"]\n",
    "    first = True\n",
    "    total_rows = 0\n",
    "    for year in years:\n",
    "        mem_before = _get_memory_bytes()\n",
    "        print(f\"Year {year}: memory before processing: {_format_bytes(mem_before)}\")\n",
    "\n",
    "        df_year = load_year(year)\n",
    "        if df_year.empty:\n",
    "            print(f\"Year {year}: no data loaded; skipping write\")\n",
    "            mem_after = _get_memory_bytes()\n",
    "            print(f\"Year {year}: memory after processing (skipped): {_format_bytes(mem_after)} delta: {_format_bytes(mem_after - mem_before)}\")\n",
    "            continue\n",
    "\n",
    "        # ensure columns are in canonical order and any missing columns are added as NA\n",
    "        df_year = df_year.reindex(columns=canonical_cols)\n",
    "        # Make the YEAR column the first column\n",
    "        cols = df_year.columns.tolist()\n",
    "        cols.insert(0, cols.pop(cols.index(\"YEAR\")))\n",
    "        df_year = df_year[cols]\n",
    "\n",
    "        print(f\"Year {year}: writing {df_year.shape[0]} rows to CSV {os.path.join(output_dir, f'brfss_{year}.csv')}\")\n",
    "        df_year.to_csv(os.path.join(output_dir, f\"brfss_{year}.csv\"), mode=\"w\", header=first, index=False)\n",
    "\n",
    "        print(f\"Year {year}: appending {df_year.shape[0]} rows to merged CSV {os.path.join(output_dir, merged_output_filename)}\")\n",
    "        df_year.to_csv(os.path.join(output_dir, merged_output_filename), mode=\"w\" if first else \"a\", header=first, index=False)\n",
    "        \n",
    "        total_rows += df_year.shape[0]\n",
    "        print(f\"[WROTE] Year {year}: {df_year.shape[0]} rows\")\n",
    "\n",
    "        # attempt to free memory and measure after write\n",
    "        del df_year\n",
    "        gc.collect()\n",
    "        mem_after = _get_memory_bytes()\n",
    "        print(f\"Year {year}: memory after processing: {_format_bytes(mem_after)} delta: {_format_bytes(mem_after - mem_before)}\")\n",
    "\n",
    "        first = False\n",
    "\n",
    "    print(f\"[DONE] {total_rows} rows written to {merged_output_filename}\")\n",
    "    return merged_output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b03353e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2015: memory before processing: 162.05 MB\n",
      "[CACHED] 2015 zip\n",
      "[CACHED] 2015 xpt\n",
      "Year 2015: 0 of 20 canonical fields missing\n",
      "Year 2015: writing 441456 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2015.csv\n",
      "Year 2015: appending 441456 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2015: 441456 rows\n",
      "Year 2015: memory after processing: 159.59 MB delta: -2.46 MB\n",
      "Year 2016: memory before processing: 159.59 MB\n",
      "[CACHED] 2016 zip\n",
      "[CACHED] 2016 xpt\n",
      "Year 2016: 0 of 20 canonical fields missing\n",
      "Year 2016: writing 486303 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2016.csv\n",
      "Year 2016: appending 486303 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2016: 486303 rows\n",
      "Year 2016: memory after processing: 161.41 MB delta: 1.81 MB\n",
      "Year 2017: memory before processing: 161.41 MB\n",
      "[CACHED] 2017 zip\n",
      "[CACHED] 2017 xpt\n",
      "Year 2017: 0 of 20 canonical fields missing\n",
      "Year 2017: writing 450016 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2017.csv\n",
      "Year 2017: appending 450016 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2017: 450016 rows\n",
      "Year 2017: memory after processing: 160.51 MB delta: -916.00 KB\n",
      "Year 2018: memory before processing: 160.51 MB\n",
      "[CACHED] 2018 zip\n",
      "[CACHED] 2018 xpt\n",
      "Year 2018: 0 of 20 canonical fields missing\n",
      "Year 2018: writing 437436 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2018.csv\n",
      "Year 2018: appending 437436 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2018: 437436 rows\n",
      "Year 2018: memory after processing: 161.75 MB delta: 1.23 MB\n",
      "Year 2019: memory before processing: 161.75 MB\n",
      "[CACHED] 2019 zip\n",
      "[CACHED] 2019 xpt\n",
      "Year 2019: 0 of 20 canonical fields missing\n",
      "Year 2019: writing 418268 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2019.csv\n",
      "Year 2019: appending 418268 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2019: 418268 rows\n",
      "Year 2019: memory after processing: 161.86 MB delta: 120.00 KB\n",
      "Year 2020: memory before processing: 161.86 MB\n",
      "[CACHED] 2020 zip\n",
      "[CACHED] 2020 xpt\n",
      "Year 2020: 0 of 20 canonical fields missing\n",
      "Year 2020: writing 401958 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2020.csv\n",
      "Year 2020: appending 401958 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2020: 401958 rows\n",
      "Year 2020: memory after processing: 163.94 MB delta: 2.07 MB\n",
      "Year 2021: memory before processing: 163.94 MB\n",
      "[CACHED] 2021 zip\n",
      "[CACHED] 2021 xpt\n",
      "Year 2021: 0 of 20 canonical fields missing\n",
      "Year 2021: writing 438693 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2021.csv\n",
      "Year 2021: appending 438693 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2021: 438693 rows\n",
      "Year 2021: memory after processing: 162.98 MB delta: -984.00 KB\n",
      "Year 2022: memory before processing: 162.98 MB\n",
      "[CACHED] 2022 zip\n",
      "[CACHED] 2022 xpt\n",
      "Year 2022: 0 of 20 canonical fields missing\n",
      "Year 2022: writing 445132 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2022.csv\n",
      "Year 2022: appending 445132 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2022: 445132 rows\n",
      "Year 2022: memory after processing: 164.00 MB delta: 1.03 MB\n",
      "Year 2023: memory before processing: 164.00 MB\n",
      "[CACHED] 2023 zip\n",
      "[CACHED] 2023 xpt\n",
      "Year 2023: 0 of 20 canonical fields missing\n",
      "Year 2023: writing 433323 rows to CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\brfss_2023.csv\n",
      "Year 2023: appending 433323 rows to merged CSV c:\\github\\brfss-diabetes-trends\\analysis\\..\\test_data_processed\\BRFSS_2015_2024.csv\n",
      "[WROTE] Year 2023: 433323 rows\n",
      "Year 2023: memory after processing: 165.05 MB delta: 1.05 MB\n",
      "[DONE] 3952585 rows written to BRFSS_2015_2024.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BRFSS_2015_2024.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_year = 2015\n",
    "end_year = 2024\n",
    "merged_output_file_name = f\"BRFSS_{start_year}_{end_year}.csv\"\n",
    "years = list(range(start_year, end_year))\n",
    "output_dir = PROCESSED_DATA_DIR\n",
    "outputfile = os.path.join(output_dir, merged_output_file_name)\n",
    "\n",
    "load_multi_year(years, output_dir, merged_output_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml_gl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
